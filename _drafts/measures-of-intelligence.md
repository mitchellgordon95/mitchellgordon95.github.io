Super-intelligent AIs probably won't look anything like we expect. Because if we
could imagine what a super-AI could do, it's within the realm of our
intelligence, and is no longer super.

It's not even clear that super-AI exists. Most people think of AI like a stat in
a game. You have *some* intelligence, and if you get more, you're smarter. Like
money. You can always get more money, and have more money.

But those people can't define intelligence, and especially not quanitfy it. It's
possible that intelligence is more like water purity. Sure, you have some water,
and it's not very pure. So you can increase the purity. But eventually, you max
out at just pure water. Intelligence might be like that, we don't know.

Like there's a universal cap on the speed things can travel in the universe
(speed of light, humans are much slower), there might be a universal cap on the
intelligence of things.

It's also unlikey, in my opinion, that we get a terminator situation. As far as
we can tell, intelligence is about two things: predicting the future, based on
the past and the present, and choosing an action to change that outcome (which
can be phrased partially in terms of the first.) So the main thing is being able
to predict the future. But that ability is not inextricably intertwined with the
primal urges for self-autonomy, or even survival. Humans leverage their
predictive capacity to survive and procreate. We are the only intelligent things
we know (and dogs and animals and stuff). This tilts our perception of what an
intelligent robot looks like towards something that pillages and consumes.

depressed robot from hitchhikers guide. Or servile creatures who only get
pleasure from being used.

Now, there could easily be someone who creates a robot that acts like us. Or
worse, only uses its intelligence to destroy. This could be done fairly easily
today (Boston dynamics, image recognition, operation and control).

How do we quanitfy intelligence? This is difficult. How do we quantify the
ability to predict? In what situations? With what goal? No two situations are
the same. But perhaps! There are only a finite number of physical laws that
dictate how things move in the universe. Then, a perfectly intelligent system,
who knows these rules, ought to be able to scale the accuracy of its predictions
with the amount of information available about the past and present. That is,
more accurate information in => more accurate predictions out.

[TODO go back and re-read your notes on static vs dynamic intelligence]

But general intelligence is more that just making one-off predictions. It
involves choosing actions, noticing when your knowledge is not enough to be
certain, and seeking out the missing information. All an ML algorithm can do is
predict. But all man can do is move. If we want the machine to be as capable as
us, it must be able to move.
