% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Shen2019-rq,
  title         = "{Q-BERT}: Hessian Based Ultra Low Precision Quantization of
                   {BERT}",
  author        = "Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and
                   Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and
                   Keutzer, Kurt",
  abstract      = "Transformer based architectures have become de-facto models
                   used for a range of Natural Language Processing tasks. In
                   particular, the BERT based models achieved significant
                   accuracy gain for GLUE tasks, CoNLL-03 and SQuAD. However,
                   BERT based models have a prohibitive memory footprint and
                   latency. As a result, deploying BERT based models in
                   resource constrained environments has become a challenging
                   task. In this work, we perform an extensive analysis of
                   fine-tuned BERT models using second order Hessian
                   information, and we use our results to propose a novel
                   method for quantizing BERT models to ultra low precision. In
                   particular, we propose a new group-wise quantization scheme,
                   and we use a Hessian based mix-precision method to compress
                   the model further. We extensively test our proposed method
                   on BERT downstream tasks of SST-2, MNLI, CoNLL-03, and
                   SQuAD. We can achieve comparable performance to baseline
                   with at most $2.3\%$ performance degradation, even with
                   ultra-low precision quantization down to 2 bits,
                   corresponding up to $13\times$ compression of the model
                   parameters, and up to $4\times$ compression of the embedding
                   table as well as activations. Among all tasks, we observed
                   the highest performance loss for BERT fine-tuned on SQuAD.
                   By probing into the Hessian based analysis as well as
                   visualization, we show that this is related to the fact that
                   current training/fine-tuning strategy of BERT does not
                   converge for SQuAD.",
  month         =  sep,
  year          =  2019,
  keywords      = "BERT Compression",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1909.05840"
}

@UNPUBLISHED{Anonymous2019-ep,
  title    = "Compressing {BERT}: Studying the Effects of Weight Pruning on
              Transfer Learning",
  author   = "{Anonymous}",
  abstract = "Universal feature extractors, such as BERT for natural language
              processing and VGG for computer vision, have become effective
              methods for improving deep learning models without requiring more
              labeled data. A common paradigm is to pre-train a feature
              extractor on large amounts of data then fine-tune it as part of a
              deep learning model on some downstream task (i.e. transfer
              learning). While effective, feature extractors like BERT may be
              prohibitively large for some deployment scenarios. We explore
              weight pruning for BERT and ask: how does compression during
              pre-training affect transfer learning? We find that pruning
              affects transfer learning in three broad regimes. Low levels of
              pruning (30-40\%) do not affect pre-training loss or transfer to
              downstream tasks at all. Medium levels of pruning increase the
              pre-training loss and prevent useful pre-training information
              from being transferred to downstream tasks. High levels of
              pruning additionally prevent models from fitting downstream
              datasets, leading to further degradation. Finally, we observe
              that fine-tuning BERT on a specific task does not improve its
              prunability. We conclude that BERT can be pruned once during
              pre-training rather than separately for each task without
              affecting performance.",
  month    =  sep,
  year     =  2019,
  keywords = "BERT Compression"
}

@ARTICLE{Sanh2019-gl,
  title         = "{DistilBERT}, a distilled version of {BERT}: smaller,
                   faster, cheaper and lighter",
  author        = "Sanh, Victor and Debut, Lysandre and Chaumond, Julien and
                   Wolf, Thomas",
  abstract      = "As Transfer Learning from large-scale pre-trained models
                   becomes more prevalent in Natural Language Processing (NLP),
                   operating these large models in on-the-edge and/or under
                   constrained computational training or inference budgets
                   remains challenging. In this work, we propose a method to
                   pre-train a smaller general-purpose language representation
                   model, called DistilBERT, which can then be fine-tuned with
                   good performances on a wide range of tasks like its larger
                   counterparts. While most prior work investigated the use of
                   distillation for building task-specific models, we leverage
                   knowledge distillation during the pre-training phase and
                   show that it is possible to reduce the size of a BERT model
                   by 40\%, while retaining 97\% of its language understanding
                   capabilities and being 60\% faster. To leverage the
                   inductive biases learned by larger models during
                   pre-training, we introduce a triple loss combining language
                   modeling, distillation and cosine-distance losses. Our
                   smaller, faster and lighter model is cheaper to pre-train
                   and we demonstrate its capabilities for on-device
                   computations in a proof-of-concept experiment and a
                   comparative on-device study.",
  month         =  oct,
  year          =  2019,
  keywords      = "BERT Compression",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1910.01108"
}

@INPROCEEDINGS{Sun2019-io,
  title           = "Patient Knowledge Distillation for {BERT} Model
                     Compression",
  booktitle       = "Proceedings of the 2019 Conference on Empirical Methods in
                     Natural Language Processing and the 9th International
                     Joint Conference on Natural Language Processing
                     ({EMNLP-IJCNLP})",
  author          = "Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing",
  abstract        = "Pre-trained language models such as BERT have proven to be
                     highly effective for natural language processing (NLP)
                     tasks. However, the high demand for computing resources in
                     training such models hinders their application in
                     practice. In order to alleviate this resource hunger in
                     large-scale model training, we propose a Patient Knowledge
                     Distillation approach to compress an original large model
                     (teacher) into an equally-effective lightweight shallow
                     network (student). Different from previous knowledge
                     distillation methods, which only use the output from the
                     last layer of the teacher network for distillation, our
                     student model patiently learns from multiple intermediate
                     layers of the teacher model for incremental knowledge
                     extraction, following two strategies: ($i$) PKD-Last:
                     learning from the last $k$ layers; and ($ii$) PKD-Skip:
                     learning from every $k$ layers. These two patient
                     distillation schemes enable the exploitation of rich
                     information in the teacher's hidden layers, and encourage
                     the student model to patiently learn from and imitate the
                     teacher through a multi-layer distillation process.
                     Empirically, this translates into improved results on
                     multiple NLP tasks with significant gain in training
                     efficiency, without sacrificing model accuracy.",
  publisher       = "Association for Computational Linguistics",
  pages           = "4314--4323",
  year            =  2019,
  address         = "Stroudsburg, PA, USA",
  keywords        = "BERT Compression",
  conference      = "Proceedings of the 2019 Conference on Empirical Methods in
                     Natural Language Processing and the 9th International
                     Joint Conference on Natural Language Processing
                     (EMNLP-IJCNLP)",
  location        = "Hong Kong, China"
}

@ARTICLE{Mukherjee2019-yj,
  title    = "Distilling Transformers into Simple Neural Networks with
              Unlabeled Transfer Data",
  author   = "Mukherjee, Subhabrata and Awadallah, Ahmed Hassan",
  abstract = "Recent advances in pre-training huge models on large amounts of
              text through self supervision have obtained state-of-the-art
              results in various natural language processing tasks. However,
              these huge and expensive models are difficult to use in practise
              for downstream tasks. Some recent efforts (Tang et al., 2019; Sun
              et al., 2019; Sanh, 2019; Turc et al., 2019) use knowledge
              distillation to compress these models. However, we see a gap
              between the performance of the smaller student models as compared
              to that of the large teacher. In this work, we leverage large
              amounts of in-domain unlabeled transfer data in addition to a
              limited amount of labeled training instances to bridge this gap.
              We show that simple RNN based student models even with hard
              distillation can perform at par with the huge teachers given the
              transfer set. The student performance can be further improved
              with soft distillation and leveraging teacher intermediate
              representations. We show that our student models can compress the
              huge teacher by up to 26x while still matching or even marginally
              exceeding the teacher performance in low-resource settings with
              small amount of labeled data.",
  journal  = "ArXiv",
  year     =  2019,
  keywords = "BERT Compression;KD, Adaptation, Regularization"
}

@ARTICLE{Scott_McCarley2019-we,
  title    = "Pruning a {BERT-based} Question Answering Model",
  author   = "Scott McCarley, J",
  abstract = "We investigate compressing a BERT-based question answering system
              by pruning parameters from the underlying BERT model. We start
              from models trained for SQuAD 2.0 and introduce gates that allow
              selected parts of transformers to be individually eliminated.
              Specifically, we investigate (1) reducing the number of attention
              heads in each transformer, (2) reducing the intermediate width of
              the feed-forward sublayer of each transformer, and (3) reducing
              the embedding dimension. We compare several approaches for
              determining the values of these gates. We find that a combination
              of pruning attention heads and the feed-forward layer almost
              doubles the decoding speed, with only a 1.5 f-point loss in
              accuracy.",
  journal  = "ArXiv",
  year     =  2019,
  keywords = "BERT Compression"
}

@MISC{Amadori2019-qb,
  title        = "Distilling knowledge from Neural Networks to build smaller
                  and faster models",
  booktitle    = "{FloydHub} Blog",
  author       = "Amadori, Alex",
  abstract     = "This article discusses GPT-2 and BERT models, as well using
                  knowledge distillation to create highly accurate models with
                  fewer parameters than their teachers",
  publisher    = "FloydHub Blog",
  month        =  nov,
  year         =  2019,
  howpublished = "\url{https://blog.floydhub.com/knowledge-distillation/}",
  note         = "Accessed: 2019-11-15",
  keywords     = "BERT Compression"
}

@UNPUBLISHED{Anonymous2019-vb,
  title    = "Reducing Transformer Depth on Demand with Structured Dropout",
  author   = "{Anonymous}",
  abstract = "Overparametrized transformer networks have obtained state of the
              art results in various natural language processing tasks, such as
              machine translation, language modeling, and question answering.
              These models contain hundreds of millions of parameters,
              necessitating a large amount of computation and making them prone
              to overfitting. In this work, we explore LayerDrop, a form of
              structured dropout, which has a regularization effect during
              training and allows for efficient pruning at inference time. In
              particular, we show that it is possible to select sub-networks of
              any depth from one large network without having to finetune them
              and with limited impact on performance. We demonstrate the
              effectiveness of our approach by improving the state of the art
              on machine translation, language modeling, summarization,
              question answering, and language understanding benchmarks.
              Moreover, we show that our approach leads to small BERT-like
              models of higher quality than when training from scratch or using
              distillation.",
  month    =  sep,
  year     =  2019,
  keywords = "BERT Compression"
}

@ARTICLE{Guo2019-ig,
  title         = "Reweighted Proximal Pruning for {Large-Scale} Language
                   Representation",
  author        = "Guo, Fu-Ming and Liu, Sijia and Mungall, Finlay S and Lin,
                   Xue and Wang, Yanzhi",
  abstract      = "Recently, pre-trained language representation flourishes as
                   the mainstay of the natural language understanding
                   community, e.g., BERT. These pre-trained language
                   representations can create state-of-the-art results on a
                   wide range of downstream tasks. Along with continuous
                   significant performance improvement, the size and complexity
                   of these pre-trained neural models continue to increase
                   rapidly. Is it possible to compress these large-scale
                   language representation models? How will the pruned language
                   representation affect the downstream multi-task transfer
                   learning objectives? In this paper, we propose Reweighted
                   Proximal Pruning (RPP), a new pruning method specifically
                   designed for a large-scale language representation model.
                   Through experiments on SQuAD and the GLUE benchmark suite,
                   we show that proximal pruned BERT keeps high accuracy for
                   both the pre-training task and the downstream multiple
                   fine-tuning tasks at high prune ratio. RPP provides a new
                   perspective to help us analyze what large-scale language
                   representation might learn. Additionally, RPP makes it
                   possible to deploy a large state-of-the-art language
                   representation model such as BERT on a series of distinct
                   devices (e.g., online servers, mobile phones, and edge
                   devices).",
  month         =  sep,
  year          =  2019,
  keywords      = "BERT Compression",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1909.12486"
}

@ARTICLE{Zafrir2019-vv,
  title         = "{Q8BERT}: Quantized 8Bit {BERT}",
  author        = "Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and
                   Wasserblat, Moshe",
  abstract      = "Recently, pre-trained Transformer based language models such
                   as BERT and GPT, have shown great improvement in many
                   Natural Language Processing (NLP) tasks. However, these
                   models contain a large amount of parameters. The emergence
                   of even larger and more accurate models such as GPT2 and
                   Megatron, suggest a trend of large pre-trained Transformer
                   models. However, using these large models in production
                   environments is a complex task requiring a large amount of
                   compute, memory and power resources. In this work we show
                   how to perform quantization-aware training during the
                   fine-tuning phase of BERT in order to compress BERT by
                   $4\times$ with minimal accuracy loss. Furthermore, the
                   produced quantized model can accelerate inference speed if
                   it is optimized for 8bit Integer supporting hardware.",
  month         =  oct,
  year          =  2019,
  keywords      = "BERT Compression",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1910.06188"
}

@ARTICLE{Wang2019-dp,
  title         = "Structured Pruning of Large Language Models",
  author        = "Wang, Ziheng and Wohlwend, Jeremy and Lei, Tao",
  abstract      = "Large language models have recently achieved state of the
                   art performance across a wide variety of natural language
                   tasks. Meanwhile, the size of these models and their latency
                   have significantly increased, which makes their usage
                   costly, and raises an interesting question: do language
                   models need to be large? We study this question through the
                   lens of model compression. We present a novel, structured
                   pruning approach based on low rank factorization and
                   augmented Lagrangian L0 norm regularization. Our structured
                   approach achieves significant inference speedups while
                   matching or outperforming our unstructured pruning baseline
                   at various sparsity levels. We apply our method to state of
                   the art models on the enwiki8 dataset and obtain a 1.19
                   perplexity score with just 5M parameters, vastly
                   outperforming a model of the same size trained from scratch.
                   We also demonstrate that our method can be applied to
                   language model fine-tuning by pruning the BERT model on
                   several downstream classification benchmarks.",
  month         =  oct,
  year          =  2019,
  keywords      = "Rubiks Method;BERT Compression",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1910.04732"
}

@ARTICLE{Lan2019-fv,
  title         = "{ALBERT}: A Lite {BERT} for Self-supervised Learning of
                   Language Representations",
  author        = "Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and
                   Gimpel, Kevin and Sharma, Piyush and Soricut, Radu",
  abstract      = "Increasing model size when pretraining natural language
                   representations often results in improved performance on
                   downstream tasks. However, at some point further model
                   increases become harder due to GPU/TPU memory limitations,
                   longer training times, and unexpected model degradation. To
                   address these problems, we present two parameter-reduction
                   techniques to lower memory consumption and increase the
                   training speed of BERT. Comprehensive empirical evidence
                   shows that our proposed methods lead to models that scale
                   much better compared to the original BERT. We also use a
                   self-supervised loss that focuses on modeling inter-sentence
                   coherence, and show it consistently helps downstream tasks
                   with multi-sentence inputs. As a result, our best model
                   establishes new state-of-the-art results on the GLUE, RACE,
                   and SQuAD benchmarks while having fewer parameters compared
                   to BERT-large.The code and the pretrained models are
                   available at
                   https://github.com/google-research/google-research/tree/master/albert.",
  month         =  sep,
  year          =  2019,
  keywords      = "\#compression;ICLR 2020;BERT Compression",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1909.11942"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@UNPUBLISHED{Anonymous2019-hz,
  title    = "{TinyBERT}: Distilling {BERT} for Natural Language Understanding",
  author   = "{Anonymous}",
  abstract = "Language model pre-training, such as BERT, has significantly
              improved the performances of many natural language processing
              tasks. However, the pre-trained language models are usually
              computationally expensive and memory intensive, so it is
              difficult to effectively execute them on resource-restricted
              devices. To accelerate inference and reduce model size while
              maintaining accuracy, we firstly propose a novel Transformer
              distillation method that is specially designed for knowledge
              distillation (KD) of the Transformer-based models. By leveraging
              this new KD method, the plenty of knowledge encoded in a large
              ``teacher'' BERT can be well transferred to a small ``student''
              TinyBERT. Moreover, we introduce a new two-stage learning
              framework for TinyBERT, which performs Transformer distillation
              at both the pre-training and task-specific learning stages. This
              framework ensures that TinyBERT can capture the general domain as
              well as the task-specific knowledge in BERT. TinyBERT is
              empirically effective and achieves comparable results with BERT
              on GLUE benchmark, while being 7.5x smaller and 9.4x faster on
              inference. TinyBERT is also significantly better than
              state-of-the-art baselines on BERT distillation, with only ∼28\%
              parameters and ∼31\% inference time of them.",
  month    =  sep,
  year     =  2019,
  keywords = "BERT Compression;ICLR 2020"
}

@UNPUBLISHED{Anonymous2019-cx,
  title    = "{MobileBERT}: {Task-Agnostic} Compression of {BERT} by
              Progressive Knowledge Transfer",
  author   = "{Anonymous}",
  abstract = "The recent development of Natural Language Processing (NLP) has
              achieved great success using large pre-trained models with
              hundreds of millions of parameters. However, these models suffer
              from the heavy model size and high latency such that we cannot
              directly deploy them to resource-limited mobile devices. In this
              paper, we propose MobileBERT for compressing and accelerating the
              popular BERT model. Like BERT, MobileBERT is task-agnostic; that
              is, it can be universally applied to various downstream NLP tasks
              via fine-tuning. MobileBERT is a slimmed version of BERT-LARGE
              augmented with bottleneck structures and a carefully designed
              balance between self-attentions and feed-forward networks. To
              train MobileBERT, we use a bottom-to-top progressive scheme to
              transfer the intrinsic knowledge of a specially designed Inverted
              Bottleneck BERT-LARGE teacher to it. Empirical studies show that
              MobileBERT is 4.3x smaller and 4.0x faster than original
              BERT-BASE while achieving competitive results on well-known NLP
              benchmarks. On the natural language inference tasks of GLUE,
              MobileBERT achieves 0.6 GLUE score performance degradation, and
              367 ms latency on a Pixel 3 phone. On the SQuAD v1.1/v2.0
              question answering task, MobileBERT achieves a 90.0/79.2 dev F1
              score, which is 1.5/2.1 higher than BERT-BASE.",
  month    =  sep,
  year     =  2019,
  keywords = "ICLR 2020;BERT Compression"
}

@ARTICLE{Zhao2019-pb,
  title         = "Extreme Language Model Compression with Optimal Subwords and
                   Shared Projections",
  author        = "Zhao, Sanqiang and Gupta, Raghav and Song, Yang and Zhou,
                   Denny",
  abstract      = "Pre-trained deep neural network language models such as
                   ELMo, GPT, BERT and XLNet have recently achieved
                   state-of-the-art performance on a variety of language
                   understanding tasks. However, their size makes them
                   impractical for a number of scenarios, especially on mobile
                   and edge devices. In particular, the input word embedding
                   matrix accounts for a significant proportion of the model's
                   memory footprint, due to the large input vocabulary and
                   embedding dimensions. Knowledge distillation techniques have
                   had success at compressing large neural network models, but
                   they are ineffective at yielding student models with
                   vocabularies different from the original teacher models. We
                   introduce a novel knowledge distillation technique for
                   training a student model with a significantly smaller
                   vocabulary as well as lower embedding and hidden state
                   dimensions. Specifically, we employ a dual-training
                   mechanism that trains the teacher and student models
                   simultaneously to obtain optimal word embeddings for the
                   student vocabulary. We combine this approach with learning
                   shared projection matrices that transfer layer-wise
                   knowledge from the teacher model to the student model. Our
                   method is able to compress the BERT\_BASE model by more than
                   60x, with only a minor drop in downstream task metrics,
                   resulting in a language model with a footprint of under 7MB.
                   Experimental results also demonstrate higher compression
                   efficiency and accuracy when compared with other
                   state-of-the-art compression techniques.",
  month         =  sep,
  year          =  2019,
  keywords      = "\#compression;BERT Compression;ICLR 2020",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1909.11687"
}

@ARTICLE{Michel2019-jc,
  title         = "Are Sixteen Heads Really Better than One?",
  author        = "Michel, Paul and Levy, Omer and Neubig, Graham",
  abstract      = "Attention is a powerful and ubiquitous mechanism for
                   allowing neural models to focus on particular salient pieces
                   of information by taking their weighted average when making
                   predictions. In particular, multi-headed attention is a
                   driving force behind many recent state-of-the-art NLP models
                   such as Transformer-based MT models and BERT. These models
                   apply multiple attention mechanisms in parallel, with each
                   attention ``head'' potentially focusing on different parts
                   of the input, which makes it possible to express
                   sophisticated functions beyond the simple weighted average.
                   In this paper we make the surprising observation that even
                   if models have been trained using multiple heads, in
                   practice, a large percentage of attention heads can be
                   removed at test time without significantly impacting
                   performance. In fact, some layers can even be reduced to a
                   single head. We further examine greedy algorithms for
                   pruning down models, and the potential speed, memory
                   efficiency, and accuracy improvements obtainable therefrom.
                   Finally, we analyze the results with respect to which parts
                   of the model are more reliant on having multiple heads, and
                   provide precursory evidence that training dynamics play a
                   role in the gains provided by multi-head attention.",
  month         =  may,
  year          =  2019,
  keywords      = "\#compression;BERT Compression",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.10650"
}
